{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.autograd as autograd\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# device = torch.device(\"cpu\")\n",
    "device = torch.device(\"cuda:0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_char_sequence(word,char_to_idx):\n",
    "    return [char_to_idx[i] for i in word]\n",
    "\n",
    "def generate_sequence(seq,word_to_idx,char_to_idx):\n",
    "#     print(word_to_idx[seq[0]],get_char_sequence(seq[0],char_to_idx))\n",
    "    return [word_to_idx[i] for i in seq]\n",
    "\n",
    "# def prepare_target(tags, pos_map):\n",
    "#     return autograd.Variable(torch.LongTensor([pos_map[i] for i in tags]))\n",
    "\n",
    "def prepare_target(tag, words_to_idx):\n",
    "    return autograd.Variable(word_to_idx[tag])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# training_data = [\n",
    "#     [\"The dog ate the apple\".lower().split(), [\"DET\", \"NN\", \"V\", \"DET\", \"NN\"]],\n",
    "#     [\"Everybody read that book\".lower().split(), [\"NN\", \"V\", \"DET\", \"NN\"]],\n",
    "#     [\"a clever brown fox jumps over the lazy dog\".lower().split(),[\"DET\",\"JJ\",\"JJ\",\"NN\",\"V\",\"IN\",\"DET\",\"JJ\",\"NN\"]]\n",
    "# ]\n",
    "from pickle import dump\n",
    "from pickle import load\n",
    "import pickle\n",
    "with open(\"telugu_pos_tag_data.pickle\",\"rb\") as fhnd:\n",
    "    total_data=pickle.load(fhnd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Shuffling section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# from random import shuffle\n",
    "\n",
    "# shuffle(total_data)\n",
    "from pickle import dump\n",
    "from pickle import load"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating word , char dictionaries , Don't run , they are created already\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Index of target task(1000 sentences) is  64078\n",
      "Ending Index of target task(1000 sentences) is  77677\n"
     ]
    }
   ],
   "source": [
    "word_to_idx,char_to_idx={},{}\n",
    "\n",
    "pos_map={}\n",
    "\n",
    "sentences=[]\n",
    "labels=[]\n",
    "\n",
    "target_task_indx_start=0\n",
    "target_task_indx_end=0\n",
    "\n",
    "length = 5\n",
    "\n",
    "\n",
    "for m,(sent,tag) in enumerate(total_data):\n",
    "    sentences+=sent\n",
    "    labels+=tag\n",
    "    \n",
    "    if m==5000:\n",
    "        print(\"Starting Index of target task(1000 sentences) is \",(len(sentences)-length))\n",
    "        target_task_indx_start=len(sentences)-length\n",
    "    if m==6000:\n",
    "        print(\"Ending Index of target task(1000 sentences) is \",len(sentences)-length)\n",
    "        target_task_indx_end=len(sentences)-length\n",
    "    \n",
    "    for j in sent:\n",
    "        if j not in word_to_idx.keys():\n",
    "            word_to_idx[j] = len(word_to_idx)\n",
    "\n",
    "        for k in j:\n",
    "            if k not in char_to_idx.keys():\n",
    "                char_to_idx[k] = len(char_to_idx)\n",
    "    \n",
    "    for tg in tag:\n",
    "        if tg not in pos_map.keys():\n",
    "            pos_map[tg] = len(pos_map)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dump(word_to_idx,open(\"word_to_idx.pickle\",\"wb\"))\n",
    "dump(char_to_idx,open(\"char_to_idx.pickle\",\"wb\"))\n",
    "dump(pos_map,open(\"pos_map.pickle\",\"wb\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run this cell to load word_to_idx , char_to_idx , pos_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "word_to_idx=load(open(\"word_to_idx.pickle\",\"rb\"))\n",
    "char_to_idx=load(open(\"char_to_idx.pickle\",\"rb\"))\n",
    "\n",
    "pos_map=load(open(\"pos_map.pickle\",\"rb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Don't run the below cells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Sequences: 103590\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "sequences = list()\n",
    "labelsequences=list()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "for i in range(length, len(sentences)):\n",
    "    # select sequence of tokens\n",
    "    seq = sentences[i-length:i+1]\n",
    "    tagseq=labels[i-length:i+1]\n",
    "    \n",
    "\n",
    "    # store\n",
    "    sequences.append(seq)\n",
    "    labelsequences.append(tagseq)\n",
    "    \n",
    "print('Total Sequences: %d' % len(sequences))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['గ్రేటర్', 'నోయిడా', 'పశ్చిమంలోని', 'మురుగునీటిని', 'తగ్గించే', 'ప్రస్తావన'], ['నోయిడా', 'పశ్చిమంలోని', 'మురుగునీటిని', 'తగ్గించే', 'ప్రస్తావన', 'చేయడం']]\n"
     ]
    }
   ],
   "source": [
    "print(sequences[:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['N-NNP', 'N-NNP', 'N_NN', 'N_NN', 'V_VM_VNF', 'N_NN'], ['N-NNP', 'N_NN', 'N_NN', 'V_VM_VNF', 'N_NN', 'V_VM_VNG']]\n"
     ]
    }
   ],
   "source": [
    "print(labelsequences[:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(['గ్రేటర్', 'నోయిడా', 'పశ్చిమంలోని', 'మురుగునీటిని', 'తగ్గించే', 'ప్రస్తావన', 'చేయడం', 'లేదు', '.'], ['N-NNP', 'N-NNP', 'N_NN', 'N_NN', 'V_VM_VNF', 'N_NN', 'V_VM_VNG', 'V_VM_VF', 'RD_PUNC']), (['దీని', 'అధీనంలో', 'మిత్రకీటకాల', 'ద్వారా', 'తెల్లపురుగుల', 'అంతం', 'చేయబడుతుంది', '.'], ['PR_PRP', 'N_NN', 'N_NN', 'PSP', 'N_NN', 'N_NN', 'V_VM_VF', 'RD_PUNC'])]\n"
     ]
    }
   ],
   "source": [
    "print(total_data[:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pickle import dump\n",
    "\n",
    "out_filename = 'tel_sent_sequences.pickle'\n",
    "\n",
    "dump(sequences,open(out_filename,\"wb\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "out_filename = 'tel_tag_sequences.pickle'\n",
    "\n",
    "dump(labelsequences,open(out_filename,\"wb\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run this to get sentences and labelsentences like \"w1,w2,w3,w4,w5\" and \"tag_1,tag_2,tag_3,tag_4,tag_5\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pickle import load\n",
    "in_filename = 'tel_sent_sequences.pickle'\n",
    "lines= load(open(in_filename,'rb'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pickle import load\n",
    "in_filename = 'tel_tag_sequences.pickle'\n",
    "labellines= load(open(in_filename,'rb'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Don't run the below 4 cells . They convert \"w1,w2,w3,w4,w5\" to \"w1_id,w2_id,w3_id,w4_id,w5_id\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sequences = list()\n",
    "for line in lines:\n",
    "\t# integer encode line\n",
    "\tencoded_seq = [word_to_idx[wrd] for wrd in line]\n",
    "\t# store\n",
    "\tsequences.append(encoded_seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pickle import dump\n",
    "out_filename = 'sequences.pickle'\n",
    "dump(sequences,open(out_filename,'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "labelsequences = list()\n",
    "for line in labellines:\n",
    "\t# integer encode line\n",
    "\tencoded_seq = [pos_map[wrd] for wrd in line]\n",
    "\t# store\n",
    "\tlabelsequences.append(encoded_seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pickle import dump\n",
    "out_filename = 'labelsequences.pickle'\n",
    "dump(sequences,open(out_filename,'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run the below cell to get vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary Size: 23138\n"
     ]
    }
   ],
   "source": [
    "\n",
    "vocab_size = len(word_to_idx)\n",
    "print('Vocabulary Size: %d' % vocab_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "103590\n",
      "103590\n"
     ]
    }
   ],
   "source": [
    "print(len(sequences))\n",
    "print(len(labelsequences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0, 1, 2, 3, 4, 5],\n",
       " [1, 2, 3, 4, 5, 6],\n",
       " [2, 3, 4, 5, 6, 7],\n",
       " [3, 4, 5, 6, 7, 8]]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sequences[:4]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Don't run the below 3 cells . These cells convert \"w1_id,w2_id,w3_id,w4_id,w5_id\" to \"w1_id,w2_id,w3_id,w4_id\"  in phase1_X and \"w5\" in phase1_y \n",
    "\n",
    "# And phase2_X contains \"w1_id,w2_id,w3_id,w4_id\" and phase2_y contains \"tag_1_id,tag_2_id,tag_3_id,tag_4_id\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy import array\n",
    "sequences = array(sequences)\n",
    "\n",
    "phase1_X=[]\n",
    "phase1_y=[]\n",
    "\n",
    "phase2_X=[]\n",
    "phase2_y=[]\n",
    "\n",
    "\n",
    "\n",
    "for sqnc,labeells in zip(sequences,labelsequences):\n",
    "    phase1_X.append(sqnc[:-1])\n",
    "    phase1_y.append(sqnc[-1])\n",
    "#     phase2_y.append(str(sqnc[-1])+\"_\"+str(labeells[-1]))\n",
    "    phase2_X.append(torch.tensor(sqnc[:-1],dtype=torch.long))\n",
    "    phase2_y.append(torch.tensor(labeells[:-1],dtype=torch.long))\n",
    "\n",
    "phase1_X=array(phase1_X)\n",
    "phase1_y=array(phase1_y)\n",
    "\n",
    "phase2_X=phase2_X\n",
    "phase2_y=phase2_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([ 0,  1,  2,  3,  4]), tensor([ 1,  2,  3,  4,  5])] [tensor([ 0,  0,  1,  1,  2]), tensor([ 0,  1,  1,  2,  1])]\n",
      "[[0 1 2 3 4]\n",
      " [1 2 3 4 5]] [5 6]\n",
      "103590\n"
     ]
    }
   ],
   "source": [
    "print(phase2_X[:2],phase2_y[:2])\n",
    "print(phase1_X[:2],phase1_y[:2])\n",
    "\n",
    "print(len(phase2_X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pickle import dump\n",
    "\n",
    "\n",
    "with open(\"phase1_X.pickle\",\"wb\") as fhand:\n",
    "    dump(phase1_X,fhand)\n",
    "\n",
    "with open(\"phase2_X.pickle\",\"wb\") as fhand:\n",
    "    dump(phase2_X,fhand)\n",
    "\n",
    "with open(\"phase1_y.pickle\",\"wb\") as fhand:\n",
    "    dump(phase1_y,fhand)\n",
    "\n",
    "with open(\"phase2_y.pickle\",\"wb\") as fhand:\n",
    "    dump(phase2_y,fhand)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run the below cell to load phase1_X,phase2_X,phase1_y,phase2_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open(\"phase1_X.pickle\",\"rb\") as fhand:\n",
    "    phase1_X=load(fhand)\n",
    "\n",
    "with open(\"phase2_X.pickle\",\"rb\") as fhand:\n",
    "    phase2_X=load(fhand)\n",
    "\n",
    "with open(\"phase1_y.pickle\",\"rb\") as fhand:\n",
    "    phase1_y=load(fhand)\n",
    "\n",
    "with open(\"phase2_y.pickle\",\"rb\") as fhand:\n",
    "    phase2_y=load(fhand)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Don't run the below 4 cells b'coz they are stored already. \"phase1_encoded_total_data\" contains [[w1_id,w2_id,w3_id,w4_id],w5_id] \n",
    "\n",
    "# \"phase2_encoded_total_data\" contains [[[w1_id,w2_id,w3_id,w4_id],[tag1_id,tag2_id,tag3_id,tag4_id]]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "phase1_encoded_total_data=[]\n",
    "for text,targ in zip(phase1_X,phase1_y):\n",
    "    phase1_encoded_total_data.append((text,targ))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "phase2_encoded_total_data=[]\n",
    "for text,targ in zip(phase2_X,phase2_y):\n",
    "    phase2_encoded_total_data.append((text,targ))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open(\"phase2_encoded_total_data.pickle\",\"wb\") as fhand:\n",
    "    dump(phase2_encoded_total_data,fhand)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open(\"phase1_encoded_total_data.pickle\",\"wb\") as fhand:\n",
    "    dump(phase1_encoded_total_data,fhand)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run the below to load phase1,2_encoded_total_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pickle import load\n",
    "with open(\"phase2_encoded_total_data.pickle\",\"rb\") as fhand:\n",
    "    phase2_encoded_total_data=load(fhand)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open(\"phase1_encoded_total_data.pickle\",\"rb\") as fhand:\n",
    "    phase1_encoded_total_data=load(fhand)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run the cells below ,to_categorical() function converts a label to one-hot vector representation.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def to_categorical(number,num_classes):\n",
    "    arr=torch.zeros((1,num_classes),dtype=torch.float)\n",
    "#     print(arr)\n",
    "    arr[0][number]=1\n",
    "    return arr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Converting the data to tensors . \n",
    "\n",
    "# [(tensor([ 0,  1,  2,  3,  4]), tensor([[ 0.,  0.,  0.,  ...,  0.,  0.,  0.]])) is one entry of phase1_encoded_catg_total_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "phase1_encoded_catg_total_data=[]\n",
    "for i,(sns,targets) in enumerate(phase1_encoded_total_data):\n",
    "    phase1_encoded_catg_total_data .append((torch.tensor(sns,dtype=torch.long),to_categorical(targets, num_classes=vocab_size)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(tensor([ 0,  1,  2,  3,  4]), tensor([[ 0.,  0.,  0.,  ...,  0.,  0.,  0.]])), (tensor([ 1,  2,  3,  4,  5]), tensor([[ 0.,  0.,  0.,  ...,  0.,  0.,  0.]]))]\n",
      "[(array([0, 1, 2, 3, 4]), 5), (array([1, 2, 3, 4, 5]), 6)]\n",
      "103590\n",
      "torch.Size([1, 23138])\n"
     ]
    }
   ],
   "source": [
    "print(phase1_encoded_catg_total_data[:2])\n",
    "print(phase1_encoded_total_data[:2])\n",
    "print(len(phase1_encoded_catg_total_data))\n",
    "\n",
    "print(phase1_encoded_catg_total_data[0][1].shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Don't run the below 5 cells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "phase1_catg_enc_training_data=phase1_encoded_catg_total_data[:len(phase1_encoded_catg_total_data)-1522]\n",
    "phase1_catg_enc_test_data=phase1_encoded_catg_total_data[-1522:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "phase2_catg_enc_training_data=phase2_encoded_total_data[:len(phase2_encoded_total_data)-1522]\n",
    "phase2_catg_enc_test_data=phase2_encoded_total_data[-1522:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch.utils.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "phase1_trainloader=torch.utils.data.DataLoader(phase1_catg_enc_training_data, batch_size=64, shuffle=False)\n",
    "phase1_testloader=torch.utils.data.DataLoader(phase1_catg_enc_test_data, batch_size=64, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# phase2_trainloader=torch.utils.data.DataLoader(phase2_catg_enc_training_data, batch_size=32, shuffle=False, num_workers=8)\n",
    "phase2_trainloader=torch.utils.data.DataLoader(phase2_catg_enc_training_data, batch_size=64, shuffle=False)\n",
    "phase2_testloader=torch.utils.data.DataLoader(phase2_catg_enc_test_data, batch_size=64, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Don't run the below 4 cells as it is creating memory error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pickle import dump\n",
    "out_filename = 'phase1_catg_enc_training_data.pickle'\n",
    "dump(phase1_catg_enc_training_data,open(out_filename,'wb'))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pickle import dump\n",
    "out_filename = 'phase1_catg_enc_test_data.pickle'\n",
    "dump(phase1_catg_enc_test_data,open(out_filename,'wb'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "from pickle import dump\n",
    "out_filename = 'phase2_catg_enc_test_data.pickle'\n",
    "dump(phase2_catg_enc_test_data,open(out_filename,'wb'))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pickle import dump\n",
    "out_filename = 'phase2_catg_enc_training_data.pickle'\n",
    "dump(phase2_catg_enc_training_data,open(out_filename,'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# from pickle import load\n",
    "# in_filename = 'phase1_catg_enc_training_data.pickle'\n",
    "# phase1_catg_enc_training_data=load(open(in_filename,'rb'))\n",
    "\n",
    "# in_filename = 'phase1_catg_enc_test_data.pickle'\n",
    "# phase1_catg_enc_test_data=load(open(in_filename,'rb'))\n",
    "\n",
    "# in_filename = 'phase2_catg_enc_test_data.pickle'\n",
    "# phase2_catg_enc_test_data=load(open(in_filename,'rb'))\n",
    "\n",
    "# in_filename = 'phase2_catg_enc_training_data.pickle'\n",
    "# phase2_catg_enc_training_data=load(open(in_filename,'rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run the below cells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "word_embedding_dim = 200\n",
    "# char_embedding_dim = 50\n",
    "hidden_dim = 450"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# pos_map={'DET':0,'NN':1,'V':2,'JJ':3,'IN':4}\n",
    "\n",
    "phase=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class pos_lstm_model(nn.Module):\n",
    "    def __init__(self,hidden_dim,embedding_dim,vocab_size,pos_tag_size):\n",
    "        super(pos_lstm_model,self).__init__()\n",
    "#         self.nn = nn.cuda()\n",
    "        \n",
    "        self.embeddings=nn.Embedding(vocab_size,embedding_dim)   # Embedding layer with output dim as 200 and input is #words\n",
    "        self.layer1 = nn.LSTM(embedding_dim,hidden_dim)          # LSTM with output dimension as 450 and input dim as 200 \n",
    "        self.layer2 = nn.Linear(hidden_dim,vocab_size)  \n",
    "        # Dense layer for predicting the next word ,input is last hidden state. Output is [1,23138]\n",
    "\n",
    "        self.phase2_layer = nn.Linear(hidden_dim,pos_tag_size) \n",
    "        # Dense layer for predicting the pos tags ,input is hidden states of the time steps and output of [#words,#pos_tags]\n",
    "        \n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.hidden = self.init_hidden() \n",
    "        \n",
    "    def init_hidden(self):\n",
    "        return (torch.zeros(1,1,hidden_dim,device=device),torch.zeros(1,1,hidden_dim,device=device))\n",
    "        \n",
    "    def forward(self,sentence):\n",
    "        \n",
    "        embed = self.embeddings(sentence) # Output Dim is [#words,#embedding_size]\n",
    "        \n",
    "        output1,self.hidden = self.layer1(embed.view(len(sentence),1,-1),self.hidden)\n",
    "        \n",
    "        # Input dim is [#words,1,#embedding_size] . Output is hidden states of each word. And self.hidden is last hidden state\n",
    "        \n",
    "        # Output dim is [#words,1,#hidden_dim] \n",
    "        \n",
    "        output2=self.layer2(output1[-1])\n",
    "        \n",
    "        #Input the last hidden state into the dense layer to predict the next word\n",
    "        \n",
    "        if phase==2:\n",
    "            phase2_output=self.phase2_layer(output1.view(len(sentence),-1))\n",
    "            # Here, we input all hidden states i,e same as #words and get output as [#words,#pos_tags]\n",
    "            \n",
    "            return F.log_softmax(output2,dim=1),F.log_softmax(phase2_output,dim=1)  \n",
    "            # Returning both predicted word and pos tags of the input words \n",
    "            \n",
    "        \n",
    "        return F.log_softmax(output2,dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = pos_lstm_model(hidden_dim,word_embedding_dim,len(word_to_idx),len(pos_map))\n",
    "model=model.cuda()\n",
    "loss_function = nn.MultiLabelSoftMarginLoss().cuda() # For Language model \n",
    "\n",
    "# loss_function = nn.BCEWithLogitsLoss().cuda()\n",
    "\n",
    "loss_function2= nn.NLLLoss().cuda()  # For POS tagging\n",
    "OPTIMIZER = optim.SGD(model.parameters(),lr = 0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "no_epochs = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "103590\n"
     ]
    }
   ],
   "source": [
    "print(len(phase1_encoded_catg_total_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch  0  of  20\n",
      "epoch  0  of  20\n",
      "epoch  1  of  20\n",
      "task1_phase2_loss is  tensor(1.00000e-04 *\n",
      "       4.6926, device='cuda:0')\n",
      "task2_phase2_loss is  tensor(0.9749, device='cuda:0')\n",
      "epoch  2  of  20\n",
      "task1_phase2_loss is  tensor(1.00000e-04 *\n",
      "       4.6895, device='cuda:0')\n",
      "task2_phase2_loss is  tensor(0.6364, device='cuda:0')\n",
      "epoch  3  of  20\n",
      "task1_phase2_loss is  tensor(1.00000e-04 *\n",
      "       4.6713, device='cuda:0')\n",
      "task2_phase2_loss is  tensor(0.2983, device='cuda:0')\n",
      "epoch  4  of  20\n",
      "task1_phase2_loss is  tensor(1.00000e-04 *\n",
      "       4.6443, device='cuda:0')\n",
      "task2_phase2_loss is  tensor(0.1748, device='cuda:0')\n",
      "epoch  5  of  20\n",
      "task1_phase2_loss is  tensor(1.00000e-04 *\n",
      "       4.6159, device='cuda:0')\n",
      "task2_phase2_loss is  tensor(0.1114, device='cuda:0')\n",
      "epoch  6  of  20\n",
      "task1_phase2_loss is  tensor(1.00000e-04 *\n",
      "       4.5873, device='cuda:0')\n",
      "task2_phase2_loss is  tensor(1.00000e-02 *\n",
      "       7.6929, device='cuda:0')\n",
      "epoch  7  of  20\n",
      "task1_phase2_loss is  tensor(1.00000e-04 *\n",
      "       4.5618, device='cuda:0')\n",
      "task2_phase2_loss is  tensor(1.00000e-02 *\n",
      "       5.8139, device='cuda:0')\n",
      "epoch  8  of  20\n",
      "task1_phase2_loss is  tensor(1.00000e-04 *\n",
      "       4.5399, device='cuda:0')\n",
      "task2_phase2_loss is  tensor(1.00000e-02 *\n",
      "       4.7666, device='cuda:0')\n",
      "epoch  9  of  20\n",
      "task1_phase2_loss is  tensor(1.00000e-04 *\n",
      "       4.5204, device='cuda:0')\n",
      "task2_phase2_loss is  tensor(1.00000e-02 *\n",
      "       4.0544, device='cuda:0')\n",
      "epoch  10  of  20\n",
      "task1_phase2_loss is  tensor(1.00000e-04 *\n",
      "       4.5023, device='cuda:0')\n",
      "task2_phase2_loss is  tensor(1.00000e-02 *\n",
      "       3.4793, device='cuda:0')\n",
      "epoch  11  of  20\n",
      "task1_phase2_loss is  tensor(1.00000e-04 *\n",
      "       4.4847, device='cuda:0')\n",
      "task2_phase2_loss is  tensor(1.00000e-02 *\n",
      "       2.9928, device='cuda:0')\n",
      "epoch  12  of  20\n",
      "task1_phase2_loss is  tensor(1.00000e-04 *\n",
      "       4.4671, device='cuda:0')\n",
      "task2_phase2_loss is  tensor(1.00000e-02 *\n",
      "       2.5766, device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "for i in range(no_epochs):  \n",
    "    if i%50 == 0:\n",
    "        print(\"epoch \", i, ' of ', no_epochs)\n",
    "        \n",
    "    print(\"epoch \", i, ' of ', no_epochs)\n",
    "        \n",
    "    if i>0:\n",
    "        print(\"task1_phase2_loss is \",task1_phase2_loss)\n",
    "        print(\"task2_phase2_loss is \",task2_phase2_loss)\n",
    "        \n",
    "    count2=0\n",
    "    for count,(sentence, tag) in enumerate(phase1_encoded_catg_total_data[:target_task_indx_end]):\n",
    "\n",
    "        \n",
    "        model.zero_grad()\n",
    "\n",
    "        model.hidden = model.init_hidden()\n",
    "\n",
    "        sentence_in=sentence.cuda() # Convert sentence tensor([w1_id,w2_id,w3_id,w4_id]) to GPU tensor\n",
    "        \n",
    "        targets=tag  # targets variable is [0,0,0,1,0,0,0,0,0,0....] for 5th indexed word in word_to_index dictionary\n",
    "        \n",
    "        \n",
    "        \n",
    "        if count>=target_task_indx_start:  # target_task_indx_start stores the index at which target data starts\n",
    "            phase=2\n",
    "            count2+=1\n",
    "            phase2_sent,phase2_tag=phase2_encoded_total_data[count] \n",
    "            \n",
    "            #phase2_sent is tensor([w1_id,w2_id,w3_id,w4_id])\n",
    "            \n",
    "            #phase2_tag is tensor([tag1_id,tag2_id,tag3_id,tag4_id])\n",
    "            \n",
    "            phase2_sent=phase2_sent.cuda()\n",
    "            phase2_tag=phase2_tag.cuda()\n",
    "            targets=targets.cuda()\n",
    "            tag_scores1,tag_scores2 = model(phase2_sent) \n",
    "            # As told in model , we have tag_scores1 storing the next word with dimension as [1,vocab_size]\n",
    "            \n",
    "            # tag_scores_2 stores the pos tags of incoming words with dimension as [#words,#pos tags]\n",
    "            \n",
    "            loss = loss_function(tag_scores1,targets)+loss_function2(tag_scores2,phase2_tag)\n",
    "            \n",
    "            #Adding both the losses\n",
    "            task1_phase2_loss=loss_function(tag_scores1,targets)\n",
    "            task2_phase2_loss=loss_function2(tag_scores2,phase2_tag)\n",
    "#             print(\"Phase 2 source task loss is\",task1_phase2_loss,\"Target task loss is\",task2_phase2_loss)\n",
    "            loss.backward()\n",
    "            \n",
    "            OPTIMIZER.step()\n",
    "            \n",
    "            \n",
    "        \n",
    "        else:\n",
    "            phase=1\n",
    "            tag_scores = model(sentence_in)\n",
    "            targets=targets.cuda()\n",
    "\n",
    "\n",
    "            phase1_loss = loss_function(tag_scores,targets)\n",
    "#             if count==0:\n",
    "#                 phase1_loss.backward(retain_graph=True)\n",
    "#             else:\n",
    "#             print(\"Phase 1 source task loss is \",phase1_loss)\n",
    "            phase1_loss.backward()\n",
    "            OPTIMIZER.step()\n",
    "            \n",
    "            \n",
    "print(\"done\")        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# for i in range(no_epochs):  \n",
    "#     if i%50 == 0:\n",
    "#         print(\"epoch \", i, ' of ', no_epochs)\n",
    "        \n",
    "#     print(\"epoch \", i, ' of ', no_epochs)\n",
    "        \n",
    "#     if i>0:\n",
    "#         print(\"Loss is \",loss)\n",
    "    \n",
    "#     count2=0\n",
    "#     for count, data in enumerate(phase1_trainloader, 0):\n",
    "        \n",
    "#         sentence, tag = data    \n",
    "# #         inputs, labels = Variable(inputs.cuda()), Variable(labels.cuda())        \n",
    "        \n",
    "#         model.zero_grad()\n",
    "\n",
    "#         model.hidden = model.init_hidden()\n",
    "\n",
    "#         sentence_in=sentence.cuda()\n",
    "        \n",
    "#         targets=tag\n",
    "        \n",
    "        \n",
    "        \n",
    "#         if count>=99999:\n",
    "#             phase=2\n",
    "#             count2+=1\n",
    "#             phase2_sent,phase2_tag=phase2_catg_enc_training_data[count]\n",
    "#             phase2_sent=phase2_sent.cuda()\n",
    "#             phase2_tag=phase2_tag.cuda()\n",
    "#             targets=targets.cuda()\n",
    "#             tag_scores1,tag_scores2 = model(phase2_sent)\n",
    "#             loss = loss_function(tag_scores1,targets)+loss_function2(tag_scores2,phase2_tag)\n",
    "#             loss.backward()\n",
    "                \n",
    "#             OPTIMIZER.step()\n",
    "            \n",
    "            \n",
    "        \n",
    "#         else:\n",
    "#             tag_scores = model(sentence_in)\n",
    "#             targets=targets.cuda()\n",
    "# #             print(targets)\n",
    "#             print(\"Tag scores is \",tag_scores[0].shape)\n",
    "#             print(tag_scores[0])\n",
    "#             loss = loss_function(tag_scores[0],targets)\n",
    "#             if count==0:\n",
    "#                 loss.backward(retain_graph=True)\n",
    "#             else:\n",
    "#                 loss.backward()\n",
    "#             OPTIMIZER.step()\n",
    "            \n",
    "            \n",
    "# print(\"done\")        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pickle import dump"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "torch.save(model,'model_LM_plus_pos_char_{}epochs.pt'.format(no_epochs))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = torch.load('model_LM_plus_pos_char_{}epochs.pt'.format(no_epochs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    \n",
    "    sentence_in = generate_sequence(training_data[1000][0], word_to_idx,char_to_idx)\n",
    "        \n",
    "    targets = prepare_target(training_data[1000][1], pos_map)\n",
    "    tag_scores = model(sentence_in)\n",
    "    print(tag_scores.shape)\n",
    "    print(targets.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "total=0\n",
    "correct=0\n",
    "for count,(sentence, tag) in enumerate(phase1_encoded_catg_total_data[:target_task_indx_end]):\n",
    "\n",
    "        \n",
    "    model.zero_grad()\n",
    "\n",
    "    model.hidden = model.init_hidden()\n",
    "\n",
    "    sentence_in=sentence.cuda() # Convert sentence tensor([w1_id,w2_id,w3_id,w4_id]) to GPU tensor\n",
    "        \n",
    "    targets=tag  # targets variable is [0,0,0,1,0,0,0,0,0,0....] for 5th indexed word in word_to_index dictionary\n",
    "        \n",
    "        \n",
    "    targets=targets.cuda()\n",
    "    tag_scores1,tag_scores2 = model(sentence) \n",
    "            # As told in model , we have tag_scores1 storing the next word with dimension as [1,vocab_size]\n",
    "            \n",
    "            # tag_scores_2 stores the pos tags of incoming words with dimension as [#words,#pos tags]\n",
    "            \n",
    "    _, predicted = torch.max(tag_scores2.data, 1)\n",
    "#     print(targets)\n",
    "#     print(predicted)\n",
    "    total += targets.size(0)\n",
    "    correct += (predicted == targets).sum().item()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for k in range(len(phase1_catg_enc_test_data)):\n",
    "        \n",
    "        tag_scores = model(sentence_in)\n",
    "        print(tag_scores.shape)\n",
    "#         print(targets.shape)\n",
    "        _, predicted = torch.max(tag_scores.data, 1)\n",
    "        \n",
    "        print(\"Predicted is \",predicted)\n",
    "        arrtargets=targets.data\n",
    "        print(\"Targets is ,\",arrtargets)\n",
    "        _,targlabel=arrtargets.max(0)\n",
    "        total += 1\n",
    "        if targlabel==predicted:\n",
    "            correct+=1\n",
    "            \n",
    "\n",
    "print('Accuracy of the network on the {} test images: %d %%'.format(len(test_data)) % (100 * correct / total))\n",
    "\n",
    "        \n",
    "# with torch.no_grad():\n",
    "#     for data in test_data:\n",
    "#         test_sent, test_tag = data\n",
    "#         sentence_in = generate_sequence(test_sent, word_to_idx,char_to_idx)\n",
    "#         targets = prepare_target(test_tag, pos_map)\n",
    "#         outputs = model(sentence_in)\n",
    "#         _, predicted = torch.max(outputs.data, 1)\n",
    "#         total += labels.size(0)\n",
    "#         correct += (predicted == labels).sum().item()\n",
    "\n",
    "# print('Accuracy of the network on the 10000 test images: %d %%' % (\n",
    "#     100 * correct / total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
